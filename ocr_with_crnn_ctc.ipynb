{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learnable Test Project\n",
    "## Task Descripition\n",
    "For this task, you are going to implement a Connectionist temporal classification(CTC) based convolutional recurrent neural netowrk(CRNN) ocr model that transcripts english handwritten images to strings. \n",
    "\n",
    "Follow the instruction below, you are going to:\n",
    "1. Build a data pipeline\n",
    "2. Implement an image encoder (most likely a convolutional neural network encoder)\n",
    "3. Build a decoder (most likely a recurrent neural network based decoder)\n",
    "4. Implement the training function to train the model \n",
    "5. Implement the validation function to validate the trained model\n",
    "\n",
    "\n",
    "To prepare yourself with above tasks, please first familiar yourself with [CRNN-CTC](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c), [pytorch CTC loss](https://pytorch.org/docs/stable/nn.html#ctcloss), [pytorch Dataset](https://pytorch.org/docs/stable/data.html), [pytorch LSTM](https://pytorch.org/docs/stable/nn.html#lstm) and [pytorch GRU](https://pytorch.org/docs/stable/nn.html#gru).\n",
    "Also here is an example for using pytorch Dataset [pytorch Dataset example](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
    "\n",
    "There are CTC-based OCR models in PyTorch out there on the internet, and you are allowed to search and reference those codes. Futhermore, you are allowed to copy paste code from those repos for this task. But you must **cite the source**. However, you are not allowed to copy paste other people's implementation in its entirety, i.e., you must follow our defined function headers to implement the model. \n",
    "\n",
    "We also provided a few helper functions. You can find them in helper functions.py. You can also create your own python files with helper functions or variables. But the main functionalities should remain in this notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please enter your name here.\n",
    "Candidate Name:Wenzheng He"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 data pipeline\n",
    "You are going to build a dataloader for your model. You can either follow the structure provided below or write your own dataloader. You should assume that loading all image samples to ram exhausts ram. So, please do not load all images into memory at once, instead, use a [online dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Discription:\n",
    "For this task, you will be given \\~30k handwritten english words, \\~5k training and \\~3k validation, with necessary infomation for you to train your OCR model. Please ignore the 'unlabeled' data.\n",
    "\n",
    "This dataset is a subset from IAM and to make the task easier, we only pick data samples whose label's length is **shorter or equal to 3**.\n",
    "\n",
    "You will be given a file: `meta_candidate.json` and a folder `data_candidate/`. \n",
    "\n",
    "`data_candidate/` contains image files for training and validation, as well as unlabeled handwritten englisht words. Each with the filename of `<img_id>.png`\n",
    "\n",
    "\n",
    "`meta_candidate.json` has the following structure: \n",
    "```text\n",
    "{\n",
    "  \"image id\":{\"bin\"(int): binarization threshhold\n",
    "              \"label\"(str): image label\n",
    "              \"path\"(str): basename for image name ('<img_id>.png')\n",
    "              \"train\"(str): label for the usage of image samples,\n",
    "                       \"train\" for training, \n",
    "                       \"val\" for validation, \n",
    "                       \"unlabeled\" is unlabeled data.\n",
    "              }\n",
    "}\n",
    "```\n",
    "\n",
    "Please DO NOT use data from anywhere else.\n",
    "\n",
    "Note that the selected subset from IAM doesn't contain certain letters and thus those letters are exincluded in the alphabet for this task. Pay attention to the variable *ALPHABET* below and the helper funcion load_lexicon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL VARIABLES\n",
    "DATA_PATH = './data/data_candidate/'\n",
    "META_PATH = './data/meta_candidate.json'\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 3 # max length of word is 3\n",
    "ALPHABET = 'abcdefghilmnoprstuwy' # only these letters will appear in data\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from helper_functions import * # take your time to check out our helpful helper functions\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class OcrDataset(Dataset):\n",
    "    def __init__(self, use='train',augment=False):\n",
    "        '''\n",
    "        dataset class for the ocr model\n",
    "\n",
    "        :param use(str): the set of data you want to load, \"train\" for training, \"val\" for evaluation\n",
    "        :param augment(bool): a flag that decides if we are going to augment data\n",
    "        '''\n",
    "        super(OcrDataset, self).__init__()\n",
    "        ##### your code here #######ctcloss\n",
    "        self.augment = augment\n",
    "        self.meta = load_json(META_PATH) #read in mata data\n",
    "        self.meta = [b for a,b in self.meta.items() \n",
    "                     if len(b['label']) > 0 and ##exclud unlabelled\n",
    "                     len(b['label']) <= 3 and ##only include upto 3 character label\n",
    "                     set(b['label']).intersection(ALPHABET) == set(b['label']) and ##make sure only include labels using selected letters\n",
    "                     b['train']==use] ##pick the correct set of data.\n",
    "        ###########################\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        return a sample, augmentation is optional. \n",
    "        :param index: index of data\n",
    "        :return: {\"img\"(numpy array or torch tensor): grayscale image with shape (1, im_height, im_width)\n",
    "                  \"label(numpy array or torch tensor)\": embedded and padded label with shape (3 ). Because by default ctc loss \n",
    "                                                        perserves 0 as padding token, so we need to increase all token \n",
    "                                                        indices by one. Ex: for label 'by', the function returns [2, 20, 0]\n",
    "                  \"len\"(int): number of characters of the sample's label.\n",
    "                  ... add anything more for your convenience\n",
    "                 }\n",
    "        '''\n",
    "        ##### your code here ######\n",
    "        token2index, _ = get_lexicon()\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_path = os.path.join(DATA_PATH,\n",
    "                                self.meta[idx]['path'])\n",
    "        img = cv2.imread(img_path) ## read in img\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) ##covert to grayscale\n",
    "        img = cv2.resize(img, (128,64), interpolation = cv2.INTER_AREA) ##resize img to 128x64 WxH close to avg W&H in the dataset.\n",
    "        img = torch.tensor(img[None,:,:]).float() ##Add batch dimension and convert to float tensor\n",
    "        label = self.meta[idx]['label'] ##get label target\n",
    "        length = len(label) ##get num of characters\n",
    "        label_token = torch.LongTensor([token2index[i]+1 for i in label] + [0]*(3-length)) ##covert token to index, pad 0 at the end\n",
    "        ###########################\n",
    "        if self.augment:\n",
    "            # augments\n",
    "            pass\n",
    "        return {'img':img,'len':length,'label':label_token}\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        total number of samples\n",
    "        :return(int): number of samples\n",
    "        '''\n",
    "        ##### your code here ######\n",
    "        return len(self.meta)\n",
    "        ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          ...,\n",
      "          [251., 251., 250.,  ..., 249., 250., 250.],\n",
      "          [250., 251., 251.,  ..., 250., 250., 250.],\n",
      "          [249., 250., 251.,  ..., 251., 251., 249.]]],\n",
      "\n",
      "\n",
      "        [[[251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          ...,\n",
      "          [251., 251., 251.,  ..., 144., 179., 179.],\n",
      "          [251., 251., 251.,  ..., 142., 176., 176.],\n",
      "          [251., 251., 251.,  ..., 200., 221., 221.]]],\n",
      "\n",
      "\n",
      "        [[[255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          ...,\n",
      "          [235., 235., 236.,  ..., 246., 245., 244.],\n",
      "          [237., 237., 239.,  ..., 249., 244., 242.],\n",
      "          [237., 238., 239.,  ..., 246., 247., 247.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          ...,\n",
      "          [251., 251., 251.,  ..., 249., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.]]],\n",
      "\n",
      "\n",
      "        [[[255., 255., 255.,  ..., 251., 250., 250.],\n",
      "          [255., 255., 255.,  ..., 251., 251., 251.],\n",
      "          [255., 255., 255.,  ..., 251., 250., 250.],\n",
      "          ...,\n",
      "          [255., 255., 255.,  ..., 250., 250., 250.],\n",
      "          [255., 255., 255.,  ..., 250., 248., 248.],\n",
      "          [255., 255., 255.,  ..., 250., 249., 249.]]],\n",
      "\n",
      "\n",
      "        [[[251., 251., 251.,  ..., 255., 255., 255.],\n",
      "          [251., 251., 251.,  ..., 255., 255., 255.],\n",
      "          [251., 251., 251.,  ..., 255., 255., 255.],\n",
      "          ...,\n",
      "          [249., 250., 250.,  ..., 246., 246., 250.],\n",
      "          [250., 251., 250.,  ..., 251., 250., 252.],\n",
      "          [251., 251., 250.,  ..., 255., 255., 255.]]]]) tensor([[19,  1, 16],\n",
      "        [13,  6,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [17, 13,  0],\n",
      "        [ 1, 16,  0],\n",
      "        [17,  8,  5],\n",
      "        [17,  8,  5],\n",
      "        [11,  1, 12],\n",
      "        [ 2, 18, 17],\n",
      "        [17,  8,  5],\n",
      "        [ 9, 16,  0],\n",
      "        [ 9, 16,  0],\n",
      "        [17,  8,  5],\n",
      "        [13,  6,  0],\n",
      "        [17,  8,  5],\n",
      "        [ 8,  9, 16],\n",
      "        [19,  8, 13],\n",
      "        [19,  1, 16],\n",
      "        [13,  6,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [17,  8,  5],\n",
      "        [17,  8,  5],\n",
      "        [17,  8,  5],\n",
      "        [ 1, 16,  0],\n",
      "        [ 1, 16,  0],\n",
      "        [17, 19, 13],\n",
      "        [ 8,  5,  0],\n",
      "        [11,  1, 20],\n",
      "        [ 9, 17,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [ 9, 17,  0],\n",
      "        [ 8,  1,  4],\n",
      "        [19,  8, 13],\n",
      "        [ 1, 12,  4],\n",
      "        [13,  6,  0],\n",
      "        [17,  8,  5],\n",
      "        [12, 13, 17],\n",
      "        [17,  8,  5],\n",
      "        [13,  6,  0],\n",
      "        [ 1, 12,  4],\n",
      "        [ 8,  1, 16],\n",
      "        [17,  8,  5],\n",
      "        [ 9, 16,  0],\n",
      "        [13,  6,  0],\n",
      "        [ 9, 17,  0],\n",
      "        [13, 15,  0],\n",
      "        [12, 13, 17],\n",
      "        [ 1, 12, 20],\n",
      "        [ 8,  1,  4],\n",
      "        [17, 13,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [17, 13,  0],\n",
      "        [13,  6,  0],\n",
      "        [17,  8,  5],\n",
      "        [ 8,  1,  4],\n",
      "        [17,  8,  5],\n",
      "        [ 9, 17, 16],\n",
      "        [ 2, 20,  0],\n",
      "        [ 8,  5, 15],\n",
      "        [19,  1, 16],\n",
      "        [ 9, 12,  0],\n",
      "        [17,  8,  5],\n",
      "        [13,  6,  0],\n",
      "        [17,  8,  5]]) tensor([3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2,\n",
      "        2, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 3,\n",
      "        3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3])\n",
      "tensor([[[[251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          ...,\n",
      "          [250., 250., 251.,  ..., 244., 244., 245.],\n",
      "          [244., 246., 251.,  ..., 244., 249., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 245., 244.]]],\n",
      "\n",
      "\n",
      "        [[[251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          ...,\n",
      "          [233., 233., 203.,  ..., 246., 250., 250.],\n",
      "          [249., 249., 247.,  ..., 250., 250., 250.],\n",
      "          [250., 250., 250.,  ..., 250., 250., 250.]]],\n",
      "\n",
      "\n",
      "        [[[242., 242., 243.,  ..., 255., 255., 255.],\n",
      "          [245., 244., 237.,  ..., 255., 255., 255.],\n",
      "          [229., 222., 170.,  ..., 255., 255., 255.],\n",
      "          ...,\n",
      "          [255., 255., 255.,  ..., 240., 240., 240.],\n",
      "          [255., 255., 255.,  ..., 243., 239., 239.],\n",
      "          [255., 255., 255.,  ..., 237., 244., 245.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[192., 192., 180.,  ..., 255., 255., 255.],\n",
      "          [171., 171., 159.,  ..., 255., 255., 255.],\n",
      "          [161., 161., 147.,  ..., 255., 255., 255.],\n",
      "          ...,\n",
      "          [255., 255., 255.,  ..., 252., 252., 252.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
      "\n",
      "\n",
      "        [[[251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 251., 251., 251.],\n",
      "          ...,\n",
      "          [251., 251., 251.,  ..., 244., 244., 244.],\n",
      "          [251., 251., 251.,  ..., 246., 244., 244.],\n",
      "          [251., 251., 251.,  ..., 249., 244., 244.]]],\n",
      "\n",
      "\n",
      "        [[[255., 255., 255.,  ..., 250., 250., 249.],\n",
      "          [255., 255., 255.,  ..., 251., 251., 250.],\n",
      "          [255., 255., 255.,  ..., 250., 251., 251.],\n",
      "          ...,\n",
      "          [255., 255., 255.,  ..., 251., 251., 250.],\n",
      "          [255., 255., 255.,  ..., 251., 251., 251.],\n",
      "          [255., 255., 255.,  ..., 251., 251., 251.]]]]) tensor([[ 8,  5,  0],\n",
      "        [ 8,  5,  0],\n",
      "        [ 9, 16,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [ 8,  1,  4],\n",
      "        [17, 13,  0],\n",
      "        [ 9, 16,  0],\n",
      "        [ 6, 13, 15],\n",
      "        [ 8,  1,  4],\n",
      "        [13, 12,  0],\n",
      "        [13,  6,  0],\n",
      "        [19,  1, 16],\n",
      "        [ 1, 12,  4],\n",
      "        [ 2, 18, 17],\n",
      "        [13,  6,  0],\n",
      "        [17,  8,  5],\n",
      "        [17,  8,  5],\n",
      "        [ 2,  5,  0],\n",
      "        [17, 13,  0],\n",
      "        [ 1, 17,  0],\n",
      "        [17, 13,  0],\n",
      "        [ 8,  9, 16],\n",
      "        [ 8,  1,  4],\n",
      "        [ 8,  5,  0],\n",
      "        [ 1, 17,  0],\n",
      "        [19,  1, 16],\n",
      "        [ 1, 12,  4],\n",
      "        [17,  8,  5],\n",
      "        [12, 13, 17],\n",
      "        [18, 14,  0],\n",
      "        [ 8,  5,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [ 4, 18,  5],\n",
      "        [ 1, 12,  4],\n",
      "        [17,  8,  5],\n",
      "        [13,  6,  0],\n",
      "        [ 8,  9, 16],\n",
      "        [ 1, 12,  4],\n",
      "        [ 1, 15,  5],\n",
      "        [17,  8,  5],\n",
      "        [17, 13,  0],\n",
      "        [13,  6,  0],\n",
      "        [ 8,  5, 15],\n",
      "        [13,  6,  0],\n",
      "        [ 8,  1, 16],\n",
      "        [17, 13,  0],\n",
      "        [19,  1, 16],\n",
      "        [ 9, 12,  0],\n",
      "        [ 8,  5,  0],\n",
      "        [13,  6,  0],\n",
      "        [17,  8,  5],\n",
      "        [ 2,  5,  0],\n",
      "        [12, 13, 17],\n",
      "        [13,  6,  0],\n",
      "        [17,  8,  5],\n",
      "        [ 1, 12,  0],\n",
      "        [17,  8,  5],\n",
      "        [ 2,  5,  0],\n",
      "        [13,  6,  0],\n",
      "        [17, 13,  0],\n",
      "        [ 1, 12,  4],\n",
      "        [ 9, 16,  0],\n",
      "        [17, 13,  0],\n",
      "        [13,  6,  0]]) tensor([2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 3, 2,\n",
      "        2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2,\n",
      "        2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2])\n",
      "tensor([[[[247., 243., 244.,  ..., 249., 248., 249.],\n",
      "          [245., 241., 241.,  ..., 248., 248., 249.],\n",
      "          [247., 244., 242.,  ..., 246., 247., 249.],\n",
      "          ...,\n",
      "          [226., 204., 172.,  ..., 240., 237., 237.],\n",
      "          [192., 144., 110.,  ..., 239., 238., 237.],\n",
      "          [141., 115., 116.,  ..., 241., 243., 242.]]],\n",
      "\n",
      "\n",
      "        [[[171., 170., 150.,  ..., 251., 251., 251.],\n",
      "          [143., 143., 145.,  ..., 251., 251., 251.],\n",
      "          [168., 166., 121.,  ..., 251., 250., 250.],\n",
      "          ...,\n",
      "          [246., 247., 248.,  ..., 249., 249., 249.],\n",
      "          [250., 250., 250.,  ..., 248., 250., 250.],\n",
      "          [250., 250., 250.,  ..., 247., 250., 250.]]],\n",
      "\n",
      "\n",
      "        [[[169., 165., 221.,  ..., 251., 251., 251.],\n",
      "          [147., 140., 188.,  ..., 251., 251., 251.],\n",
      "          [153., 142., 158.,  ..., 251., 251., 251.],\n",
      "          ...,\n",
      "          [251., 251., 251.,  ..., 250., 251., 251.],\n",
      "          [251., 251., 251.,  ..., 248., 249., 251.],\n",
      "          [251., 251., 251.,  ..., 250., 251., 251.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          ...,\n",
      "          [251., 251., 248.,  ..., 251., 251., 251.],\n",
      "          [255., 255., 255.,  ..., 251., 251., 251.],\n",
      "          [255., 255., 255.,  ..., 251., 251., 251.]]],\n",
      "\n",
      "\n",
      "        [[[230., 230., 230.,  ..., 255., 255., 255.],\n",
      "          [198., 198., 198.,  ..., 255., 255., 255.],\n",
      "          [107., 107., 107.,  ..., 255., 255., 255.],\n",
      "          ...,\n",
      "          [255., 255., 255.,  ..., 254., 254., 254.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
      "\n",
      "\n",
      "        [[[255., 255., 255.,  ..., 241., 241., 241.],\n",
      "          [255., 255., 255.,  ..., 232., 236., 238.],\n",
      "          [255., 255., 255.,  ..., 217., 228., 234.],\n",
      "          ...,\n",
      "          [236., 236., 236.,  ..., 239., 239., 239.],\n",
      "          [238., 237., 236.,  ..., 241., 239., 238.],\n",
      "          [237., 236., 235.,  ..., 253., 253., 252.]]]]) tensor([[17,  8,  5],\n",
      "        [17, 13,  0],\n",
      "        [17, 19, 13],\n",
      "        [ 2, 18, 17],\n",
      "        [13,  6,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [ 2, 18, 17],\n",
      "        [ 8,  9, 11],\n",
      "        [16,  9, 17],\n",
      "        [ 9, 17,  0],\n",
      "        [ 6, 13, 15],\n",
      "        [ 1, 16,  0],\n",
      "        [20, 13, 18],\n",
      "        [ 6, 13, 15],\n",
      "        [ 9, 12,  0],\n",
      "        [16, 13,  0],\n",
      "        [19,  1, 16],\n",
      "        [ 1, 15,  5],\n",
      "        [ 9, 12,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [ 8,  1,  4],\n",
      "        [ 8,  5, 15],\n",
      "        [17,  8,  5],\n",
      "        [17,  8,  5],\n",
      "        [17,  8,  5],\n",
      "        [14, 18, 17],\n",
      "        [ 8,  1,  4],\n",
      "        [ 9, 16,  0],\n",
      "        [13,  6,  0],\n",
      "        [ 1, 17,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [17, 13, 13],\n",
      "        [17,  8,  5],\n",
      "        [17,  8,  5],\n",
      "        [ 5, 20,  5],\n",
      "        [16,  8,  5],\n",
      "        [ 8,  1,  4],\n",
      "        [17, 13,  0],\n",
      "        [ 1, 12,  0],\n",
      "        [ 8,  1,  4],\n",
      "        [17,  8,  5],\n",
      "        [17,  8,  5],\n",
      "        [ 8,  5, 15],\n",
      "        [10,  1,  7],\n",
      "        [13, 15,  0],\n",
      "        [ 9, 12,  0],\n",
      "        [17, 13,  0],\n",
      "        [ 1, 12,  4],\n",
      "        [ 1, 16,  0],\n",
      "        [11,  1, 20],\n",
      "        [17, 13,  0],\n",
      "        [ 1, 17,  0],\n",
      "        [ 8,  5, 15],\n",
      "        [ 8,  9, 16],\n",
      "        [13,  6,  0],\n",
      "        [ 8,  1,  4],\n",
      "        [ 8,  1,  4],\n",
      "        [17,  8,  5],\n",
      "        [ 2,  5,  0],\n",
      "        [12, 13, 17],\n",
      "        [ 9,  6,  0],\n",
      "        [ 9, 16,  0],\n",
      "        [ 9, 16,  0],\n",
      "        [13,  6,  0]]) tensor([3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3,\n",
      "        3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 3,\n",
      "        2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "ocr_dataset_train = OcrDataset('train')\n",
    "data_loader_train = DataLoader(ocr_dataset_train, batch_size = BATCH_SIZE, shuffle=True)\n",
    "ocr_dataset_eval = OcrDataset('val')\n",
    "data_loader_eval= DataLoader(ocr_dataset_eval, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for i,sample in enumerate(data_loader_train):\n",
    "    print(sample['img'],sample['label'],sample['len'])\n",
    "    if i >1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convolutional Recurrent Neural Network Model\n",
    "You are going to implement a convolutional recurrent neural network model. Follow the instruction below to build a encder and a decoder. Then combine them to get the ocr model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Encoder\n",
    "Implement any image encoder you like for the CTC model. You can reference existing ones. Note we only want resonable accuracy for this task. So, please consider inference speed when designing the model architecture. Overly complex CNN model would also increase your training time and thus leave you with less time to tune the model. \n",
    "\n",
    "You should try to guarantee that the height dimension for output feature map is 1, otherwise, you'll need to deal with this problem in decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### your code here ######\n",
    "# input to encoder is a batch of samples(batch_size, 1, im_height, im_width)\n",
    "class CTCEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CTCEncoder, self).__init__()\n",
    "        ##set some parameters for cnn\n",
    "        ks = [3, 3, 3, 3, 3, 3, 3, 3] ##kernel\n",
    "        ps = [1, 1, 1, 1, 1, 1, 1, 0] ##padding\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1, 1] ##stride\n",
    "        nm = [32, 64, 128, 256, 256, 512, 512, 512] ##feature map depth\n",
    "\n",
    "        ##define cnn structure\n",
    "        cnn = nn.Sequential()\n",
    "        \n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = 1 if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)                                                 # 32x64x128\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 32x32x64\n",
    "        convRelu(1)                                                 # 64x32x64\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 64x16x32\n",
    "        convRelu(2, True)                                           # 128x16x32\n",
    "        convRelu(3)                                                 # 256x16x32\n",
    "        cnn.add_module('pooling{0}'.format(2), nn.MaxPool2d(2, 2))  # 256x8x16\n",
    "        convRelu(4, True)                                           # 256x8x16\n",
    "        convRelu(5)                                                 # 512x8x16\n",
    "        cnn.add_module('pooling{0}'.format(3), nn.MaxPool2d((2, 2),(2, 1),(0,1)))  # 512x4x17\n",
    "        convRelu(6, True)                                                          # 512x4x17\n",
    "        convRelu(7)                                                                # 512x2x15\n",
    "        cnn.add_module('pooling{0}'.format(4), nn.MaxPool2d((2, 2),(2, 1),(0,1)))  # 512x1x16\n",
    "        \n",
    "        self.cnn = cnn\n",
    "    def forward(self, feature):\n",
    "        return self.cnn(feature)\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:torch.Size([1, 1, 64, 128])\n",
      "output shape:torch.Size([1, 512, 1, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "##test input/output shape\n",
    "en_test = CTCEncoder()\n",
    "test = torch.tensor(ocr_dataset_train[100][\"img\"][None,:,:,:]).float()\n",
    "result = en_test.forward(test)\n",
    "print(\"input shape:{}\".format(test.shape))\n",
    "print(\"output shape:{}\".format(result.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Decoder\n",
    "implement a Decoder that takes cnn encoded features and output loglogits. In CRNN model, the decoder is most likely RNN based. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCDecoder(nn.Module):\n",
    "    '''\n",
    "    decoder for the transcription model\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CTCDecoder, self).__init__()\n",
    "        ##### your code here ######\n",
    "        self.nh = 256\n",
    "        self.nOut = len(ALPHABET)+1\n",
    "        self.rnn = nn.LSTM(512, self.nh, bidirectional=True)\n",
    "        self.fc = nn.Linear(self.nh * 2, self.nOut)\n",
    "        ###########################\n",
    "    def forward(self, feature):\n",
    "        '''\n",
    "        forward function for decoding submodule\n",
    "\n",
    "        :param feature(torch tensor): encoded feature with shape (batch_size, hidden_dim, 1, feature_map_width) \n",
    "        :return: loglogits(torch tensor) for each block on the image(feature_map_width, batch_size, lexicon_size) \n",
    "        '''\n",
    "        ##### your code here ######\n",
    "        feature = feature.squeeze(2) #remove height dimension from CNN output\n",
    "        feature = feature.permute(2, 0, 1) #move feature_map_width to the first dimension\n",
    "        recurrent, _ = self.rnn(feature)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h) ##concat bidiretional lstm results\n",
    "\n",
    "        output = self.fc(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        output = output.log_softmax(2)\n",
    "        ###########################\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:torch.Size([1, 512, 1, 16])\n",
      "output:torch.Size([16, 1, 21])\n"
     ]
    }
   ],
   "source": [
    "##test input/output shape\n",
    "de_test = CTCDecoder()\n",
    "result_de = de_test.forward(result)\n",
    "print(\"input:{}\\noutput:{}\".format(result.shape,result_de.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Combine Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Crnn(\n",
       "  (encoder): CTCEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu0): ReLU(inplace=True)\n",
       "      (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu3): ReLU(inplace=True)\n",
       "      (pooling2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batchnorm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu4): ReLU(inplace=True)\n",
       "      (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu5): ReLU(inplace=True)\n",
       "      (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       "      (conv6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu6): ReLU(inplace=True)\n",
       "      (conv7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (relu7): ReLU(inplace=True)\n",
       "      (pooling4): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): CTCDecoder(\n",
       "    (rnn): LSTM(512, 256, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=21, bias=True)\n",
       "  )\n",
       "  (loss): CTCLoss()\n",
       ")"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import CTCLoss #https://pytorch.org/docs/stable/nn.html#ctcloss\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "USE_GPU=True\n",
    "LR = 1e-3\n",
    "\n",
    "class Crnn(nn.Module):\n",
    "    '''\n",
    "    module for the whole transcription model, crnn stands for\n",
    "    convolutional recurrent network\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Crnn, self).__init__()\n",
    "        ##### fill in #####\n",
    "        self.encoder = CTCEncoder()\n",
    "        self.decoder = CTCDecoder()\n",
    "        self.loss = CTCLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = LR)\n",
    "        ################### \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        forward function for crnn model, converts a batch of\n",
    "        images to a batch of sequences of log logits.\n",
    "        :param x(pytorch tensor): a tensor of images (batch_size, 1, im_height, im_width)\n",
    "        :return: loglogits(pytorch tensor) with shape(feature_map_width, batch_size, lexicon_size) \n",
    "        '''\n",
    "        encoded_feature = self.encoder(x)\n",
    "        loglogits = self.decoder(encoded_feature)\n",
    "        return loglogits\n",
    "\n",
    "\n",
    "Crnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:torch.Size([1, 1, 64, 128])\n",
      "output:torch.Size([16, 1, 21])\n"
     ]
    }
   ],
   "source": [
    "##test input/output shape\n",
    "model_test = Crnn()\n",
    "result_crnn = model_test(test)\n",
    "print(\"input:{}\\noutput:{}\".format(test.shape,result_crnn.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:torch.Size([64, 1, 64, 128])\n",
      "output:torch.Size([16, 64, 21])\n"
     ]
    }
   ],
   "source": [
    "#test input/output for one batch\n",
    "sample_test = next(iter(data_loader_train))\n",
    "result_crnn = model_test(sample_test['img'])\n",
    "print(\"input:{}\\noutput:{}\".format(sample_test['img'].shape, result_crnn.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##encoder/decoder reference source:https://github.com/foamliu/CRNN/blob/master/models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, sample):\n",
    "    '''\n",
    "    train function for transcription model\n",
    "\n",
    "    :param model(pytorch module): pytorch transcription model\n",
    "    :param sample(dict): a minibatch of samples, the dict should at least include\n",
    "                        keys 'img', 'label', 'len'\n",
    "    :return: loss(float) and accuracy(float) for current train step\n",
    "    '''\n",
    "    # get loglogits\n",
    "    model.train() #set mode of model\n",
    "    x = sample['img']\n",
    "    if USE_GPU:\n",
    "        x.cuda()\n",
    "    loglogits = model(x)\n",
    "    \n",
    "    ##### fill in the code to format output, get loss and back propergate #####\n",
    "    # check #https://pytorch.org/docs/stable/nn.html#ctcloss for document for CTC loss\n",
    "\n",
    "    #format target and output\n",
    "    batch_size = loglogits.shape[1]\n",
    "    target_lengths = torch.LongTensor((sample['len']))\n",
    "    targets = sample['label']\n",
    "    if USE_GPU:\n",
    "        targets.cuda()\n",
    "    output_lengths = torch.autograd.Variable(torch.IntTensor([loglogits.size(0)] * batch_size))\n",
    "    \n",
    "    #get loss\n",
    "    loss = model.loss(loglogits, targets, output_lengths, target_lengths)\n",
    "    \n",
    "    #back prop and update weights\n",
    "    model.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    model.optimizer.step()\n",
    "    #####\n",
    "    \n",
    "    accy = calc_accy(loglogits, sample['label'], sample['len'])\n",
    "    return loss.item(), accy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, sample):\n",
    "    '''\n",
    "    validation function for transcription model\n",
    "\n",
    "    :param model(pytorch module): pytorch transcription model\n",
    "    :param sample(dict): a minibatch of samples, the dict should at least include\n",
    "                        keys 'img', 'label', 'len'\n",
    "    :return: accuracy(float) for validation step\n",
    "    '''\n",
    "    ##### fill in to code to forward and get validation output #####\n",
    "    model.eval() #set to eval mode\n",
    "    x = sample['img']\n",
    "    if USE_GPU:\n",
    "        x.cuda()\n",
    "    logits = model(x)\n",
    "    labels = sample['label']\n",
    "    label_lengths = sample['len']\n",
    "    ################################################################\n",
    "    accy = calc_accy(logits, labels, label_lengths)\n",
    "    return accy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Expermenting\n",
    "Using the functions above to create your training script. Tune hyperparameters and do any experiment and analysis you like. The traing accuracy should >90% and validation accuracy should >50%. Feel free to add any other features, such as training curve visualization, checkpoing loading and saving, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(n_epochs, model, start_epoch = 0):\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        ##variable to track loss/acc\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        valid_acc = 0.0\n",
    "        ##train\n",
    "        for i,sample in enumerate(data_loader_train):\n",
    "            loss_item, acc = train(model, sample)\n",
    "            train_loss += loss_item\n",
    "            train_acc += acc\n",
    "        ##valid\n",
    "        for i,sample in enumerate(data_loader_eval):\n",
    "            acc = validate(model, sample)\n",
    "            valid_acc += acc\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining acc: {:.6f} \\tValidation acc: {:.6f}'.format(\n",
    "            start_epoch + epoch, \n",
    "            train_loss/len(data_loader_train),\n",
    "            train_acc/len(data_loader_train),\n",
    "            valid_acc/len(data_loader_eval),\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 3.307678 \tTraining acc: 0.000000 \tValidation acc: 0.000000\n",
      "Epoch: 2 \tTraining Loss: 2.717684 \tTraining acc: 0.000000 \tValidation acc: 0.000000\n",
      "Epoch: 3 \tTraining Loss: 2.138598 \tTraining acc: 0.000000 \tValidation acc: 0.000000\n",
      "Epoch: 4 \tTraining Loss: 1.539362 \tTraining acc: 0.093924 \tValidation acc: 0.046474\n",
      "Epoch: 5 \tTraining Loss: 1.045012 \tTraining acc: 0.401975 \tValidation acc: 0.151415\n",
      "Epoch: 6 \tTraining Loss: 0.768983 \tTraining acc: 0.573351 \tValidation acc: 0.415175\n",
      "Epoch: 7 \tTraining Loss: 0.576011 \tTraining acc: 0.666862 \tValidation acc: 0.322696\n",
      "Epoch: 8 \tTraining Loss: 0.461980 \tTraining acc: 0.733398 \tValidation acc: 0.514630\n",
      "Epoch: 9 \tTraining Loss: 0.351848 \tTraining acc: 0.790039 \tValidation acc: 0.497638\n",
      "Epoch: 10 \tTraining Loss: 0.272768 \tTraining acc: 0.834983 \tValidation acc: 0.477758\n"
     ]
    }
   ],
   "source": [
    "#### free style time #####\n",
    "model = Crnn()\n",
    "#first train 10 epochs\n",
    "train_loop(10, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 0.211068 \tTraining acc: 0.871853 \tValidation acc: 0.534096\n",
      "Epoch: 12 \tTraining Loss: 0.169552 \tTraining acc: 0.899544 \tValidation acc: 0.374516\n",
      "Epoch: 13 \tTraining Loss: 0.123459 \tTraining acc: 0.927127 \tValidation acc: 0.554404\n",
      "Epoch: 14 \tTraining Loss: 0.112462 \tTraining acc: 0.938368 \tValidation acc: 0.576053\n",
      "Epoch: 15 \tTraining Loss: 0.080453 \tTraining acc: 0.954102 \tValidation acc: 0.604098\n"
     ]
    }
   ],
   "source": [
    "#train 5 more epochs\n",
    "train_loop(5, model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 \tTraining Loss: 0.055026 \tTraining acc: 0.968338 \tValidation acc: 0.547925\n",
      "Epoch: 17 \tTraining Loss: 0.074274 \tTraining acc: 0.956467 \tValidation acc: 0.613589\n",
      "Epoch: 18 \tTraining Loss: 0.057719 \tTraining acc: 0.964149 \tValidation acc: 0.610342\n",
      "Epoch: 19 \tTraining Loss: 0.036333 \tTraining acc: 0.979687 \tValidation acc: 0.619543\n",
      "Epoch: 20 \tTraining Loss: 0.023815 \tTraining acc: 0.987305 \tValidation acc: 0.627943\n",
      "Epoch: 21 \tTraining Loss: 0.016887 \tTraining acc: 0.993164 \tValidation acc: 0.631258\n",
      "Epoch: 22 \tTraining Loss: 0.011599 \tTraining acc: 0.995703 \tValidation acc: 0.640238\n",
      "Epoch: 23 \tTraining Loss: 0.008070 \tTraining acc: 0.997070 \tValidation acc: 0.639520\n",
      "Epoch: 24 \tTraining Loss: 0.009193 \tTraining acc: 0.996289 \tValidation acc: 0.614114\n",
      "Epoch: 25 \tTraining Loss: 0.012915 \tTraining acc: 0.992383 \tValidation acc: 0.580391\n"
     ]
    }
   ],
   "source": [
    "#train 10 more see if we can get any better\n",
    "train_loop(10, model, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##seems like we might start to overfit the training data.\n",
    "##save the current model\n",
    "torch.save(model.state_dict(), \"model_crnn.pt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
